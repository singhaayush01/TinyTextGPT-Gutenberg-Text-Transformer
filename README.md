 Gutenberg Text Transformer ğŸ§ 

A from-scratch implementation of a Transformer-based language model trained on text from Project Gutenberg.  
This project is part of my hands-on journey to understand how Transformers (like GPT) work at the architecture level.

---

 ğŸ§­ Overview
The goal of this project is to build a miniature GPT-style model step-by-step â€” from raw text to a trained Transformer that can generate sentences.  
Each step adds one major concept, focusing on clarity and code transparency over scale.

---

 ğŸªœ 10-Step Roadmap

1. Get the Data â€“ Download and clean text datasets from Project Gutenberg using Pythonâ€™s `requests`.  
2. Train the Tokenizer â€“ Implement a Byte-Pair Encoding (BPE) tokenizer to convert text into subword tokens.  
3. Positional Encoding â€“ Add Rotary Positional Encoding (RoPE) so the model understands word order.  
4. Grouped Query Attention (GQA) â€“ Implement the attention mechanism that powers all modern Transformers.  
5. Causal Masking â€“ Ensure the model only â€œlooks backwardâ€ during training, not ahead at future tokens.  
6. Feed-Forward (MoE) Layer â€“ Add a Mixture-of-Experts (MoE) layer for efficient non-linear transformations.  
7. Normalization & Skip Connections â€“ Stabilize training with RMSNorm and residual links.  
8. Full Transformer Block â€“ Stack all modules (Attention + MLP + Norm) into a complete decoder block.  
9. Training Loop â€“ Train the model with PyTorch using self-supervised next-token prediction.  
10. Text Generation â€“ Generate text using sampling or nucleus decoding, and visualize results.

---

 âœ… Progress So Far

- âœ… Step 1: Downloaded and cleaned dataset from Project Gutenberg  
- âœ… Step 2: Trained BPE tokenizer and saved `gutenberg_tokenizer.json`  
- âœ… Step 3: Implemented Rotary Positional Encoding  
- â¬œ Step 4: Building Grouped Query Attention module  
- â¬œ Step 5â€“10: Coming soon ğŸš€

