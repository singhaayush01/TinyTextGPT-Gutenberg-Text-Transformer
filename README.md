# Gutenberg Text Transformer ðŸ§ 

A from-scratch implementation of a Transformer-based language model trained on classic books from Project Gutenberg.  
This project is part of my hands-on journey to understand how Transformer models like GPT work from the ground up.

---

## Overview
This project explores the process of building a small Transformer model step-by-step â€” from collecting text data and training a tokenizer to implementing the Transformer architecture and generating text.

---

## Project Steps

1. Downloaded and cleaned dataset from Project Gutenberg  
2. Trained a Byte-Pair Encoding (BPE) tokenizer  
3. (Upcoming) Implement Transformer architecture  
4. (Upcoming) Train model and generate text outputs  

---

## Tools and Libraries
- Python 3.9+
- Requests â€“ for downloading datasets  
- Hugging Face Tokenizers â€“ for tokenizer training  
- PyTorch â€“ for model building and training (coming next)

---

## Project Structure
